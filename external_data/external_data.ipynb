{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data from external source into storage, preprocess, train\n",
    "\n",
    "#process:    \n",
    "#data : in external source (remote), download data into dkube   \n",
    "#dkube_preprocess_op : data in external source (remote), download data into dkube and do some preprocessing\n",
    "#dkube_storage_op : store the downloaded data in dkube storage (mount) as pv volume\n",
    "#dkube_training_op: take preprocess data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import json\n",
    "from dkube.sdk import *\n",
    "from dkube.sdk import DkubeApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_url = \"/mnt/dkube/pipeline/components/\"\n",
    "storage_op = kfp.components.load_component_from_file(\"/mnt/dkube/pipeline/components/storage/component.yaml\")\n",
    "dkube_preprocessing_op = kfp.components.load_component_from_file(components_url + \"preprocess/component.yaml\")\n",
    "token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"ocdr/d3-datascience-sklearn:v0.23.2-1\"\n",
    "code_name = \"external-data\"\n",
    "dataset=\"heart-data\"\n",
    "ptrain_dataset = \"heart-data\"\n",
    "train_fs_name = \"heart-fs-train\"\n",
    "dataset_mount_points = [\"/opt/dkube/input\"]\n",
    "featureset_mount_points = [\"/featureset/train\"]\n",
    "preprocessing_script = f\"python external_data/preprocessing.py --train_fs {train_fs_name}\"\n",
    "model_name = \"heart-model\"\n",
    "training_script = \"python external_data/training.py\"\n",
    "train_out_mount_points = [\"/model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = DkubeApi(token=token)\n",
    "api.create_featureset(DkubeFeatureSet(train_fs_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='external_data',\n",
    "    description='utilise data from external and train'\n",
    ")\n",
    "def externaldata_pipeline(token,code_name,dataset):\n",
    "     with kfp.dsl.ExitHandler(exit_op=storage_op(\"reclaim\", token, namespace=\"kubeflow\", uid=\"{{workflow.uid}}\")):\n",
    "            \n",
    "            preprocessing = dkube_preprocessing_op(token, json.dumps({\"image\": image}),\n",
    "                                            program = code_name,run_script=preprocessing_script,\n",
    "                                            datasets=json.dumps([ptrain_dataset]), \n",
    "                                            output_featuresets=json.dumps([train_fs_name]),\n",
    "                                            input_dataset_mounts=json.dumps(dataset_mount_points), \n",
    "                                            output_featureset_mounts=json.dumps(featureset_mount_points))\n",
    "            \n",
    "            input_volumes = json.dumps([\"{{workflow.uid}}-featureset@featureset://\" + train_fs_name])\n",
    "            storage = storage_op(\"export\",token, namespace=\"kubeflow\", input_volumes=input_volumes,\n",
    "                                 output_volumes=json.dumps([\"{{workflow.uid}}-featureset@featureset://\"+train_fs_name])).after(preprocessing)\n",
    "            \n",
    "            \n",
    "            train = dkube_training_op(token, json.dumps({\"image\": image}),\n",
    "                                    framework=\"sklearn\", version=\"0.23.2\",\n",
    "                                    program=code_name, run_script=training_script,\n",
    "                                    featuresets=json.dumps([train_fs_name]), outputs=json.dumps([model_name]),\n",
    "                                    input_featureset_mounts=json.dumps(output_volumes),\n",
    "                                    output_mounts=json.dumps(train_out_mount_points)).after(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(existing_token=token)\n",
    "client.create_run_from_pipeline_func(externaldata_pipeline,arguments={\"token\":token,\"code_name\":code_name,\"dataset\":dataset})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
