{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time,kfp,json\n",
    "from dkube.sdk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "username =  os.getenv(\"USERNAME\")\n",
    "dkube_url = os.getenv(\"DKUBE_URL\")\n",
    "api = DkubeApi(URL=dkube_url,token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_name = generate(\"dkube-examples\")\n",
    "code = DkubeCode(username, name=code_name)\n",
    "code.update_git_details(\"https://github.com/oneconvergence/dkube-examples.git\")\n",
    "api.create_code(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = generate(\"mnist\")\n",
    "dataset = DkubeDataset(username, name=dataset_name)\n",
    "dataset.update_git_details(\"https://dkube.s3.amazonaws.com/datasets/mnist.pkl.gz\")\n",
    "dataset.update_dataset_source(source='pub_url')\n",
    "api.create_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = generate(\"mnist\")\n",
    "model = DkubeModel(username, name=model_name)\n",
    "model.update_model_source(source='dvs')\n",
    "api.create_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_op = kfp.components.load_component_from_file(\"/mnt/dkube/pipeline/components/storage/component.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name='StorageOp',description='StorageOp component')\n",
    "\n",
    "def storageop_artifacts():\n",
    "    with kfp.dsl.ExitHandler(exit_op=storage_op(\"reclaim\", token, namespace=\"kubeflow\",uid=\"{{workflow.uid}}\")):\n",
    "            input_volumes = json.dumps([\"{{workflow.uid}}-project@program://\"+str(code_name),\"{{workflow.uid}}-dataset@dataset://\"+str(dataset_name),\"{{workflow.uid}}-model@model://\" + str(model_name)])\n",
    "            storage  = storage_op(\"export\", token, namespace=\"kubeflow\", input_volumes=input_volumes)\n",
    "            \n",
    "            train = kfp.dsl.ContainerOp(\n",
    "                name=\"container-op\",\n",
    "                image=\"docker.io/ocdr/dkube-datascience-tf-cpu:v2.0.0-3\",\n",
    "                command=\"bash\", \n",
    "                arguments=[\"-c\", \"ls /dataset\"],\n",
    "                pvolumes={\n",
    "                         \"/project\": kfp.dsl.PipelineVolume(pvc=\"{{workflow.uid}}-project\"),\n",
    "                         \"/dataset\": kfp.dsl.PipelineVolume(pvc=\"{{workflow.uid}}-dataset\"),\n",
    "                         \"/model\": kfp.dsl.PipelineVolume(pvc=\"{{workflow.uid}}-model\")\n",
    "                         }).after(storage)\n",
    "           \n",
    "            \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(existing_token=token)\n",
    "client.create_run_from_pipeline_func(storageop_artifacts,arguments={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
